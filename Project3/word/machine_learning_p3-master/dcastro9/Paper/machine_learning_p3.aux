\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{Castro1:2013}
\citation{Castro2:2013}
\citation{Castro1:2013}
\citation{Hall_weka:2010}
\citation{Castro1:2013}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background Information}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Clustering Algorithms}{1}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}K-Means Clustering}{1}{subsubsection.2.1.1}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Expectation Maximization (EM)}{1}{subsubsection.2.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Dimensionality Reduction Algorithms}{1}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Principal Component Analysis (PCA)}{1}{subsubsection.2.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Independent Component Analysis (ICA)}{1}{subsubsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Random Projections (RP)}{1}{subsubsection.2.2.3}}
\citation{Castro1:2013}
\citation{elter2007prediction}
\citation{Castro1:2013}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Random Subsets (RS)}{2}{subsubsection.2.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Datasets}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Wisconsin Breast Cancer (Diagnostic Dataset)}{2}{subsubsection.2.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Mammographic Mass Dataset}{2}{subsubsection.2.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Clustering Algorithms}{2}{section.3}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}K-Means Clustering}{2}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces This illustrates that the Manhattan and Euclidean distances had negligible effects, but in both instances the Wisconsin Dataset outperformed the Mammography dataset, which can be explained due to the disparity in the number of attributes between the datasets.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:clust1}{{1}{2}{This illustrates that the Manhattan and Euclidean distances had negligible effects, but in both instances the Wisconsin Dataset outperformed the Mammography dataset, which can be explained due to the disparity in the number of attributes between the datasets.\relax \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Expectation Maximization (EM)}{2}{subsection.3.2}}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces This demonstrates that the performance of EM without a target number of clusters tends to return erroneous results. The results for both of the datasets, assuming I supply a cluster size of 2, are not experimentally significant in comparison to simple k-means. In this figure's key, C? represents the act of not specifying a cluster size and letting the algorithm determine it.\relax }}{3}{figure.caption.3}}
\newlabel{fig:clust2}{{2}{3}{This demonstrates that the performance of EM without a target number of clusters tends to return erroneous results. The results for both of the datasets, assuming I supply a cluster size of 2, are not experimentally significant in comparison to simple k-means. In this figure's key, C? represents the act of not specifying a cluster size and letting the algorithm determine it.\relax \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Dimensionality Reduction Algorithms}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}PCA}{3}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}ICA}{3}{subsection.4.2}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Random Projections}{3}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Random Subsets}{3}{subsection.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Clustering with Dimensionality Reduction}{3}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}K-Means Clustering}{3}{subsection.5.1}}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This demonstrates the accuracy after performing clustering and dimensionality reduction. for k means. The best result out of the Manhattan and Euclidean distance were picked.\relax }}{4}{figure.caption.4}}
\newlabel{fig:kmeanscluster}{{3}{4}{This demonstrates the accuracy after performing clustering and dimensionality reduction. for k means. The best result out of the Manhattan and Euclidean distance were picked.\relax \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Expectation Maximization (EM)}{4}{subsection.5.2}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Effect of number of attributes on computing time\relax }}{4}{figure.caption.5}}
\newlabel{fig:timevsattr}{{4}{4}{Effect of number of attributes on computing time\relax \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Clustering after Dimensionality Reduction Results for Expectation Maximization (EM)\relax }}{4}{figure.caption.6}}
\newlabel{fig:clustdimred2}{{5}{4}{Clustering after Dimensionality Reduction Results for Expectation Maximization (EM)\relax \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Neural Network Learner}{4}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Dimensionality Reduction}{4}{subsection.6.1}}
\citation{Frank+Asuncion:2010}
\citation{elter2007prediction}
\citation{Hall_weka:2010}
\bibstyle{acmsiggraph}
\bibdata{machine_learning_p3}
\bibcite{Castro2:2013}{\citename {Castro }2013a}
\bibcite{Castro1:2013}{\citename {Castro }2013b}
\bibcite{elter2007prediction}{\citename {Elter et\nobreakspace  {}al\unhbox \voidb@x \hbox {.} }2007}
\bibcite{Frank+Asuncion:2010}{\citename {Frank and Asuncion }2010}
\bibcite{Hall_weka:2010}{\citename {Hall et\nobreakspace  {}al\unhbox \voidb@x \hbox {.} }2010}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The increase in computing time is caused by the increased number of parameters in ICA. This figure augments Figure \nobreakspace  {}\ref  {fig:timevsattr} which demonstrates the number of attributes affects the computing time. Although the time is increased, it comes with the benefit of accuracy, which is a classic tradeoff in computing problems.\relax }}{5}{figure.caption.7}}
\newlabel{fig:perftime}{{6}{5}{The increase in computing time is caused by the increased number of parameters in ICA. This figure augments Figure ~\ref {fig:timevsattr} which demonstrates the number of attributes affects the computing time. Although the time is increased, it comes with the benefit of accuracy, which is a classic tradeoff in computing problems.\relax \relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of Algorithm Performance - None is the ground truth. In this case, the original dataset performes better, but its important to highlight that PCA and RS run at significantly faster rates for a small tradeoff in accuracy.\relax }}{5}{table.caption.8}}
\newlabel{tb:perfdesc}{{1}{5}{Comparison of Algorithm Performance - None is the ground truth. In this case, the original dataset performes better, but its important to highlight that PCA and RS run at significantly faster rates for a small tradeoff in accuracy.\relax \relax }{table.caption.8}{}}
\newlabel{tb:perf}{{1}{5}{Comparison of Algorithm Performance - None is the ground truth. In this case, the original dataset performes better, but its important to highlight that PCA and RS run at significantly faster rates for a small tradeoff in accuracy.\relax \relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Clustering with Dimensionality Reduction}{5}{subsection.6.2}}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{5}{section.7}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Future Work}{5}{section.8}}
