\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Markov Decision Processes Chosen}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Problem instances}{1}{subsection.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Medium map. Goal state in orange.}}{1}{figure.1}}
\newlabel{fig:mediummap}{{1}{1}{Medium map. Goal state in orange}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Large map. Goal state in orange.}}{1}{figure.2}}
\newlabel{fig:bigmap}{{2}{1}{Large map. Goal state in orange}{figure.2}{}}
\newlabel{eq:mdptransition}{{1}{1}{Problem instances}{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}MDP Interest}{2}{subsection.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Phase 1 - Value Iteration/Policy Iteration}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Value Iteration}{2}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Value Iteration policy for medium map with $p=0.2$}}{2}{figure.3}}
\newlabel{fig:valueiter02}{{3}{2}{Value Iteration policy for medium map with $p=0.2$}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Value Iteration policy for medium map with $p=0.8$}}{2}{figure.4}}
\newlabel{fig:valueiter08}{{4}{2}{Value Iteration policy for medium map with $p=0.8$}{figure.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Value iteration results for both maps. Note that $p$ is the probability of state transition error.}}{3}{table.1}}
\newlabel{table:valueitertable}{{1}{3}{Value iteration results for both maps. Note that $p$ is the probability of state transition error}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Iterations to convergence for Value Iteration for both maps.}}{3}{figure.5}}
\newlabel{fig:valueiterationiterations}{{5}{3}{Iterations to convergence for Value Iteration for both maps}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Runtime to convergence for Value Iteration for both maps.}}{3}{figure.6}}
\newlabel{fig:valueiterationruntime}{{6}{3}{Runtime to convergence for Value Iteration for both maps}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Policy Iteration}{4}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Policy Iteration policy for medium map with $p=0.2$}}{4}{figure.7}}
\newlabel{fig:policyiter02}{{7}{4}{Policy Iteration policy for medium map with $p=0.2$}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Policy Iteration policy for medium map with $p=0.8$}}{4}{figure.8}}
\newlabel{fig:policyiter08}{{8}{4}{Policy Iteration policy for medium map with $p=0.8$}{figure.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Policy iteration results for both maps. Note that $p$ is the probability of state transition error.}}{4}{table.2}}
\newlabel{table:policyitertable}{{2}{4}{Policy iteration results for both maps. Note that $p$ is the probability of state transition error}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Iterations to convergence for Policy Iteration for both maps.}}{5}{figure.9}}
\newlabel{fig:policyiterationiterations}{{9}{5}{Iterations to convergence for Policy Iteration for both maps}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Runtime to convergence for Policy Iteration for both maps.}}{5}{figure.10}}
\newlabel{fig:policyiterationruntime}{{10}{5}{Runtime to convergence for Policy Iteration for both maps}{figure.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Asymptotic behavior of algorithms explored by examining the percentage increase in runtime from the medium map to the large map.}}{5}{table.3}}
\newlabel{table:valueitervspolicyiter}{{3}{5}{Asymptotic behavior of algorithms explored by examining the percentage increase in runtime from the medium map to the large map}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Value iteration versus Policy Iteration}{5}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Phase 2 - Q-Learning and Prioritized Sweeping}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Q-Learning}{6}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Parameter Hyperspace Analysis Experiment}{6}{subsubsection.3.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Parameter Hyperspace Analysis Experiment Results}{6}{subsubsection.3.1.2}}
\newlabel{qlscatterbigsouce}{{4}{6}{}{Hfootnote.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Q-learning policy after 10,000 iterations with $p=0.3$, $\epsilon =0.1$}}{7}{figure.11}}
\newlabel{fig:qlpolicy0301}{{11}{7}{Q-learning policy after 10,000 iterations with $p=0.3$, $\epsilon =0.1$}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Q-learning policy after 10,000 iterations with $p=0.7$, $\epsilon =0.1$}}{7}{figure.12}}
\newlabel{fig:qlpolicy0301}{{12}{7}{Q-learning policy after 10,000 iterations with $p=0.7$, $\epsilon =0.1$}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Q-Learning scores for the medium map over a space of $p$ and $\epsilon $ values, visualized as a 3D scatter plot.}}{7}{figure.13}}
\newlabel{fig:qlscatter}{{13}{7}{Q-Learning scores for the medium map over a space of $p$ and $\epsilon $ values, visualized as a 3D scatter plot}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Q-Learning scores for the medium map over a space of $p$ and $\epsilon $ values, visualized as a heatmap.}}{7}{figure.14}}
\newlabel{fig:qlheatmap}{{14}{7}{Q-Learning scores for the medium map over a space of $p$ and $\epsilon $ values, visualized as a heatmap}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces $\epsilon $ vs runtime for the medium map for Q-learning. An exponential regression is superimposed on the graph.}}{8}{figure.15}}
\newlabel{fig:qlruntimemedium}{{15}{8}{$\epsilon $ vs runtime for the medium map for Q-learning. An exponential regression is superimposed on the graph}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Q-Learning scores for the big map over a space of $p$ and $\epsilon $ values, visualized as a 3D scatter plot.}}{8}{figure.16}}
\newlabel{fig:qlscatterbig}{{16}{8}{Q-Learning scores for the big map over a space of $p$ and $\epsilon $ values, visualized as a 3D scatter plot}{figure.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Comparision of Q-learning with model-based learners}{8}{subsubsection.3.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Prioritized Sweeping}{9}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Relationship between $p$, $\epsilon $ and score for the medium map with Prioritized Sweeping.}}{9}{figure.17}}
\newlabel{fig:psscore}{{17}{9}{Relationship between $p$, $\epsilon $ and score for the medium map with Prioritized Sweeping}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Relationship between $p$, $\epsilon $ and runtime for the medium map with Prioritized Sweeping.}}{9}{figure.18}}
\newlabel{fig:psruntime}{{18}{9}{Relationship between $p$, $\epsilon $ and runtime for the medium map with Prioritized Sweeping}{figure.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Effect of $p$ and $\epsilon $ on score}{9}{subsubsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Effect of $p$ and $\epsilon $ on runtime}{10}{subsubsection.3.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{10}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Value Iteration}{10}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Policy Iteration}{10}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Q-learning}{10}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Prioritized Sweeping}{10}{subsection.4.4}}
\newlabel{LastPage}{{}{10}{}{page.10}{}}
\xdef\lastpage@lastpage{10}
\xdef\lastpage@lastpageHy{10}
